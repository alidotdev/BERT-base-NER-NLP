{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mloyeHhj6PyX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==2.4.1\n",
      "  Downloading transformers-2.4.1-py3-none-any.whl (475 kB)\n",
      "                                              0.0/475.8 kB ? eta -:--:--\n",
      "     --                                      30.7/475.8 kB 1.3 MB/s eta 0:00:01\n",
      "     -----                                 71.7/475.8 kB 787.7 kB/s eta 0:00:01\n",
      "     -----------                            143.4/475.8 kB 1.1 MB/s eta 0:00:01\n",
      "     ------------------                     235.5/475.8 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------               307.2/475.8 kB 1.4 MB/s eta 0:00:01\n",
      "     --------------------------------       409.6/475.8 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------  471.0/475.8 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------  471.0/475.8 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 475.8/475.8 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (1.24.3)\n",
      "Collecting tokenizers==0.0.11 (from transformers==2.4.1)\n",
      "  Downloading tokenizers-0.0.11.tar.gz (30 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (1.24.28)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (3.9.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (2022.7.9)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\muhammad ali murtaza\\appdata\\roaming\\python\\python311\\site-packages (from transformers==2.4.1) (0.1.99)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==2.4.1) (0.0.43)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==2.4.1) (0.4.6)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.4.1) (1.27.59)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.4.1) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->transformers==2.4.1) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==2.4.1) (2023.5.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.4.1) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.4.1) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers==2.4.1) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers==2.4.1) (2.8.2)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [20 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uiXKXxE4x7g-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Muhammad Ali Murtaza\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Muhammad Ali Murtaza\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertConfig, BertTokenizer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertForWordClassification\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mforward_fn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_word_classification\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ner_metrics_fn\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'modules'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from modules.word_classification import BertForWordClassification\n",
    "from utils.forward_fn import forward_word_classification\n",
    "from utils.metrics import ner_metrics_fn\n",
    "from utils.data_utils import IndQurNerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7s1TLGVE1UM"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCylQ7jPFVrK"
   },
   "outputs": [],
   "source": [
    "def word_subword_tokenize(sentence, tokenizer):\n",
    "    # Add CLS token\n",
    "    subwords = [tokenizer.cls_token_id]\n",
    "    subword_to_word_indices = [-1] # For CLS\n",
    "\n",
    "    # Add subwords\n",
    "    for word_idx, word in enumerate(sentence):\n",
    "        subword_list = tokenizer.encode(word, add_special_tokens=False)\n",
    "        subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "        subwords += subword_list\n",
    "\n",
    "    # Add last SEP token\n",
    "    subwords += [tokenizer.sep_token_id]\n",
    "    subword_to_word_indices += [-1]\n",
    "\n",
    "    return subwords, subword_to_word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGef2wfN2PT2"
   },
   "outputs": [],
   "source": [
    "# common functions\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)\n",
    "\n",
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = IndQurNerDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForWordClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)\n",
    "w2i, i2w = IndQurNerDataset.LABEL2INDEX, IndQurNerDataset.INDEX2LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9us-ZJ8_JYeP"
   },
   "source": [
    "# Fine-tuning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtkpAdr9GU3i"
   },
   "outputs": [],
   "source": [
    "train_dataset_path = 'datasets/train.txt'\n",
    "valid_dataset_path = 'datasets/dev.txt'\n",
    "test_dataset_path = 'datasets/test.txt'\n",
    "\n",
    "train_dataset = IndQurNerDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "valid_dataset = IndQurNerDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "test_dataset = IndQurNerDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = NerDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=True)  \n",
    "valid_loader = NerDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)  \n",
    "test_loader = NerDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=16, num_workers=16, shuffle=False)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "model = model.cuda()\n",
    "# Train\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        None\n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDCQ4sp18pmk"
   },
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model on test set\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    _, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    list_hyp += batch_hyp\n",
    "    list_label += batch_label\n",
    "\n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "df.to_csv('pred.txt', index=False)\n",
    "\n",
    "print(df)\n",
    "\n",
    "metrics = ner_metrics_fn(list_hyp, list_label)\n",
    "print(metrics_to_string(metrics))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
